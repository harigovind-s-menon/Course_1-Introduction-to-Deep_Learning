{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "toc": {
      "colors": {
        "hover_highlight": "#DAA520",
        "navigate_num": "#000000",
        "navigate_text": "#333333",
        "running_highlight": "#FF0000",
        "selected_highlight": "#FFD700",
        "sidebar_border": "#EEEEEE",
        "wrapper_background": "#FFFFFF"
      },
      "moveMenuLeft": true,
      "nav_menu": {
        "height": "264px",
        "width": "252px"
      },
      "navigate_menu": true,
      "number_sections": true,
      "sideBar": true,
      "threshold": 4,
      "toc_cell": false,
      "toc_section_display": "block",
      "toc_window_display": false,
      "widenNotebook": false
    },
    "colab": {
      "name": "NumpyNN (honor).ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harigovind-s-menon/intro-to-dl/blob/master/week2/Programming%20Assignment%202%20-%20Numpy%20Neural%20networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqHtsbPO4Xge",
        "colab_type": "text"
      },
      "source": [
        "### Your very own neural network\n",
        "\n",
        "In this notebook we're going to build a neural network using naught but pure numpy and steel nerves. It's going to be fun, I promise!\n",
        "\n",
        "<img src=\"https://github.com/harigovind-s-menon/intro-to-dl/blob/master/week2/frankenstein.png?raw=1\" style=\"width:20%\">"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ITaMFuZpv0u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "9e87576f-bc9d-48ee-a5fa-7eb9b93cad2f"
      },
      "source": [
        "! shred -u setup_google_colab.py\n",
        "! wget https://raw.githubusercontent.com/hse-aml/intro-to-dl/master/setup_google_colab.py -O setup_google_colab.py\n",
        "import setup_google_colab\n",
        "# please, uncomment the week you're working on\n",
        "setup_google_colab.setup_week2()"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-07-24 04:26:47--  https://raw.githubusercontent.com/hse-aml/intro-to-dl/master/setup_google_colab.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3792 (3.7K) [text/plain]\n",
            "Saving to: ‘setup_google_colab.py’\n",
            "\n",
            "\rsetup_google_colab.   0%[                    ]       0  --.-KB/s               \rsetup_google_colab. 100%[===================>]   3.70K  --.-KB/s    in 0s      \n",
            "\n",
            "2019-07-24 04:26:47 (54.7 MB/s) - ‘setup_google_colab.py’ saved [3792/3792]\n",
            "\n",
            "**************************************************\n",
            "inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "**************************************************\n",
            "cifar-10-batches-py.tar.gz\n",
            "**************************************************\n",
            "mnist.npz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRlOTDRk4Xgk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.append(\"..\")\n",
        "import tqdm_utils\n",
        "import download_utils\n",
        "import pandas as pd\n",
        "import time "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2QR_iGrsqYL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def eval_numerical_gradient(f, x, verbose=False, h=0.00001):\n",
        "    \"\"\"Evaluates gradient df/dx via finite differences:\n",
        "    df/dx ~ (f(x+h) - f(x-h)) / 2h\n",
        "    Adopted from https://github.com/ddtm/dl-course/ (our ysda course).\n",
        "    \"\"\"\n",
        "    fx = f(x) # evaluate function value at original point\n",
        "    grad = np.zeros_like(x)\n",
        "    # iterate over all indexes in x\n",
        "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
        "    while not it.finished:\n",
        "\n",
        "        # evaluate function at x+h\n",
        "        ix = it.multi_index\n",
        "        oldval = x[ix]\n",
        "        x[ix] = oldval + h # increment by h\n",
        "        fxph = f(x) # evalute f(x + h)\n",
        "        x[ix] = oldval - h\n",
        "        fxmh = f(x) # evaluate f(x - h)\n",
        "        x[ix] = oldval # restore\n",
        "\n",
        "        # compute the partial derivative with centered formula\n",
        "        grad[ix] = (fxph - fxmh) / (2 * h) # the slope\n",
        "        if verbose:\n",
        "            print (ix, grad[ix])\n",
        "        it.iternext() # step to next dimension\n",
        "\n",
        "    return grad"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ePCW7ZF4Xgp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# use the preloaded keras datasets and models\n",
        "download_utils.link_all_keras_resources()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1yKmRwH4Xgs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function\n",
        "import numpy as np\n",
        "np.random.seed(42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1GQPW-i4Xgw",
        "colab_type": "text"
      },
      "source": [
        "Here goes our main class: a layer that can do .forward() and .backward() passes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXuAWlSD4Xgx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Layer:\n",
        "    \"\"\"\n",
        "    A building block. Each layer is capable of performing two things:\n",
        "    \n",
        "    - Process input to get output:           output = layer.forward(input)\n",
        "    \n",
        "    - Propagate gradients through itself:    grad_input = layer.backward(input, grad_output)\n",
        "    \n",
        "    Some layers also have learnable parameters which they update during layer.backward.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        \"\"\"Here you can initialize layer parameters (if any) and auxiliary stuff.\"\"\"\n",
        "        # A dummy layer does nothing\n",
        "        pass\n",
        "    \n",
        "    def forward(self, input):\n",
        "        \"\"\"\n",
        "        Takes input data of shape [batch, input_units], returns output data [batch, output_units]\n",
        "        \"\"\"\n",
        "        # A dummy layer just returns whatever it gets as input.\n",
        "        return input\n",
        "\n",
        "    def backward(self, input, grad_output):\n",
        "        \"\"\"\n",
        "        Performs a backpropagation step through the layer, with respect to the given input.\n",
        "        \n",
        "        To compute loss gradients w.r.t input, you need to apply chain rule (backprop):\n",
        "        \n",
        "        d loss / d x  = (d loss / d layer) * (d layer / d x)\n",
        "        \n",
        "        Luckily, you already receive d loss / d layer as input, so you only need to multiply it by d layer / d x.\n",
        "        \n",
        "        If your layer has parameters (e.g. dense layer), you also need to update them here using d loss / d layer\n",
        "        \"\"\"\n",
        "        # The gradient of a dummy layer is precisely grad_output, but we'll write it more explicitly\n",
        "        num_units = input.shape[1]\n",
        "        \n",
        "        d_layer_d_input = np.eye(num_units)\n",
        "        \n",
        "        return np.dot(grad_output, d_layer_d_input) # chain rule"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkJhhZud4Xg1",
        "colab_type": "text"
      },
      "source": [
        "### The road ahead\n",
        "\n",
        "We're going to build a neural network that classifies MNIST digits. To do so, we'll need a few building blocks:\n",
        "- Dense layer - a fully-connected layer, $f(X)=W \\cdot X + \\vec{b}$\n",
        "- ReLU layer (or any other nonlinearity you want)\n",
        "- Loss function - crossentropy\n",
        "- Backprop algorithm - a stochastic gradient descent with backpropageted gradients\n",
        "\n",
        "Let's approach them one at a time.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hU8JpaZW4Xg2",
        "colab_type": "text"
      },
      "source": [
        "### Nonlinearity layer\n",
        "\n",
        "This is the simplest layer you can get: it simply applies a nonlinearity to each element of your network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EV6LOovw4Xg3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ReLU(Layer):\n",
        "    def __init__(self):\n",
        "        \"\"\"ReLU layer simply applies elementwise rectified linear unit to all inputs\"\"\"\n",
        "        pass\n",
        "    \n",
        "    def forward(self, input):\n",
        "        \"\"\"Apply elementwise ReLU to [batch, input_units] matrix\"\"\"\n",
        "        # <your code. Try np.maximum>\n",
        "        relu_fwd = np.maximum(input, 0)\n",
        "        return relu_fwd\n",
        "    \n",
        "    def backward(self, input, grad_output):\n",
        "        \"\"\"Compute gradient of loss w.r.t. ReLU input\"\"\"\n",
        "        relu_grad = input > 0\n",
        "        return grad_output*relu_grad        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MJzM2fu4Xg-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        },
        "outputId": "4bdf4dcd-ea79-4ca1-f145-57c2cec179f8"
      },
      "source": [
        "# some tests\n",
        "x = np.linspace(-1,1,10*32).reshape([10,32])\n",
        "l = ReLU()\n",
        "grads = l.backward(x,np.ones([10,32])/(32*10))\n",
        "numeric_grads = eval_numerical_gradient(lambda x: l.forward(x).mean(), x=x)\n",
        "assert np.allclose(grads, numeric_grads, rtol=1e-3, atol=0),\\\n",
        "    \"gradient returned by your layer does not match the numerically computed gradient\"\n",
        "print(numeric_grads)"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.       0.       0.       0.       0.       0.       0.       0.\n",
            "  0.       0.       0.       0.       0.       0.       0.       0.\n",
            "  0.       0.       0.       0.       0.       0.       0.       0.\n",
            "  0.       0.       0.       0.       0.       0.       0.       0.      ]\n",
            " [0.       0.       0.       0.       0.       0.       0.       0.\n",
            "  0.       0.       0.       0.       0.       0.       0.       0.\n",
            "  0.       0.       0.       0.       0.       0.       0.       0.\n",
            "  0.       0.       0.       0.       0.       0.       0.       0.      ]\n",
            " [0.       0.       0.       0.       0.       0.       0.       0.\n",
            "  0.       0.       0.       0.       0.       0.       0.       0.\n",
            "  0.       0.       0.       0.       0.       0.       0.       0.\n",
            "  0.       0.       0.       0.       0.       0.       0.       0.      ]\n",
            " [0.       0.       0.       0.       0.       0.       0.       0.\n",
            "  0.       0.       0.       0.       0.       0.       0.       0.\n",
            "  0.       0.       0.       0.       0.       0.       0.       0.\n",
            "  0.       0.       0.       0.       0.       0.       0.       0.      ]\n",
            " [0.       0.       0.       0.       0.       0.       0.       0.\n",
            "  0.       0.       0.       0.       0.       0.       0.       0.\n",
            "  0.       0.       0.       0.       0.       0.       0.       0.\n",
            "  0.       0.       0.       0.       0.       0.       0.       0.      ]\n",
            " [0.003125 0.003125 0.003125 0.003125 0.003125 0.003125 0.003125 0.003125\n",
            "  0.003125 0.003125 0.003125 0.003125 0.003125 0.003125 0.003125 0.003125\n",
            "  0.003125 0.003125 0.003125 0.003125 0.003125 0.003125 0.003125 0.003125\n",
            "  0.003125 0.003125 0.003125 0.003125 0.003125 0.003125 0.003125 0.003125]\n",
            " [0.003125 0.003125 0.003125 0.003125 0.003125 0.003125 0.003125 0.003125\n",
            "  0.003125 0.003125 0.003125 0.003125 0.003125 0.003125 0.003125 0.003125\n",
            "  0.003125 0.003125 0.003125 0.003125 0.003125 0.003125 0.003125 0.003125\n",
            "  0.003125 0.003125 0.003125 0.003125 0.003125 0.003125 0.003125 0.003125]\n",
            " [0.003125 0.003125 0.003125 0.003125 0.003125 0.003125 0.003125 0.003125\n",
            "  0.003125 0.003125 0.003125 0.003125 0.003125 0.003125 0.003125 0.003125\n",
            "  0.003125 0.003125 0.003125 0.003125 0.003125 0.003125 0.003125 0.003125\n",
            "  0.003125 0.003125 0.003125 0.003125 0.003125 0.003125 0.003125 0.003125]\n",
            " [0.003125 0.003125 0.003125 0.003125 0.003125 0.003125 0.003125 0.003125\n",
            "  0.003125 0.003125 0.003125 0.003125 0.003125 0.003125 0.003125 0.003125\n",
            "  0.003125 0.003125 0.003125 0.003125 0.003125 0.003125 0.003125 0.003125\n",
            "  0.003125 0.003125 0.003125 0.003125 0.003125 0.003125 0.003125 0.003125]\n",
            " [0.003125 0.003125 0.003125 0.003125 0.003125 0.003125 0.003125 0.003125\n",
            "  0.003125 0.003125 0.003125 0.003125 0.003125 0.003125 0.003125 0.003125\n",
            "  0.003125 0.003125 0.003125 0.003125 0.003125 0.003125 0.003125 0.003125\n",
            "  0.003125 0.003125 0.003125 0.003125 0.003125 0.003125 0.003125 0.003125]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePoXwe5k4XhB",
        "colab_type": "text"
      },
      "source": [
        "#### Instant primer: lambda functions\n",
        "\n",
        "In python, you can define functions in one line using the `lambda` syntax: `lambda param1, param2: expression`\n",
        "\n",
        "For example: `f = lambda x, y: x+y` is equivalent to a normal function:\n",
        "\n",
        "```\n",
        "def f(x,y):\n",
        "    return x+y\n",
        "```\n",
        "For more information, click [here](http://www.secnetix.de/olli/Python/lambda_functions.hawk).    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eh2Y4HzQ4XhD",
        "colab_type": "text"
      },
      "source": [
        "### Dense layer\n",
        "\n",
        "Now let's build something more complicated. Unlike nonlinearity, a dense layer actually has something to learn.\n",
        "\n",
        "A dense layer applies affine transformation. In a vectorized form, it can be described as:\n",
        "$$f(X)= W \\cdot X + \\vec b $$\n",
        "\n",
        "Where \n",
        "* X is an object-feature matrix of shape [batch_size, num_features],\n",
        "* W is a weight matrix [num_features, num_outputs] \n",
        "* and b is a vector of num_outputs biases.\n",
        "\n",
        "Both W and b are initialized during layer creation and updated each time backward is called."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LbGSWuXJ4XhF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Dense(Layer):\n",
        "    def __init__(self, input_units, output_units, learning_rate=0.1):\n",
        "        \"\"\"\n",
        "        A dense layer is a layer which performs a learned affine transformation:\n",
        "        f(x) = <W*x> + b\n",
        "        \"\"\"\n",
        "        self.learning_rate = learning_rate\n",
        "        \n",
        "        # initialize weights with small random numbers. We use normal initialization, \n",
        "        # but surely there is something better. Try this once you got it working: http://bit.ly/2vTlmaJ\n",
        "        self.weights = np.random.randn(input_units, output_units)*0.01\n",
        "        self.biases = np.zeros(output_units)\n",
        "        \n",
        "    def forward(self,input):\n",
        "        \"\"\"\n",
        "        Perform an affine transformation:\n",
        "        f(x) = <W*x> + b\n",
        "        \n",
        "        input shape: [batch, input_units]\n",
        "        output shape: [batch, output units]\n",
        "        \"\"\"\n",
        "        \n",
        "        #return #<your code here>\n",
        "        return np.dot(input, self.weights) + self.biases\n",
        "    \n",
        "    def backward(self,input,grad_output):\n",
        "        \n",
        "        # compute d f / d x = d f / d dense * d dense / d x\n",
        "        # where d dense/ d x = weights transposed\n",
        "        #grad_input = #<your code here>\n",
        "        grad_input = np.dot(grad_output, self.weights.T)\n",
        "        \n",
        "        # compute gradient w.r.t. weights and biases\n",
        "        #grad_weights = #<your code here>\n",
        "        #grad_biases = #<your code here>\n",
        "        grad_weights = np.dot(input.T, grad_output)\n",
        "        grad_biases = grad_output.mean(axis=0)*input.shape[0]\n",
        "        \n",
        "        assert grad_weights.shape == self.weights.shape and grad_biases.shape == self.biases.shape\n",
        "        # Here we perform a stochastic gradient descent step. \n",
        "        # Later on, you can try replacing that with something better.\n",
        "        self.weights = self.weights - self.learning_rate * grad_weights\n",
        "        self.biases = self.biases - self.learning_rate * grad_biases\n",
        "        \n",
        "        return grad_input"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDEjv3EI4XhN",
        "colab_type": "text"
      },
      "source": [
        "### Testing the dense layer\n",
        "\n",
        "Here we have a few tests to make sure your dense layer works properly. You can just run them, get 3 \"well done\"s and forget they ever existed.\n",
        "\n",
        "... or not get 3 \"well done\"s and go fix stuff. If that is the case, here are some tips for you:\n",
        "* Make sure you compute gradients for W and b as __sum of gradients over batch__, not mean over gradients. Grad_output is already divided by batch size.\n",
        "* If you're debugging, try saving gradients in class fields, like \"self.grad_w = grad_w\" or print first 3-5 weights. This helps debugging.\n",
        "* If nothing else helps, try ignoring tests and proceed to network training. If it trains alright, you may be off by something that does not affect network training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5OM_DlRY4XhP",
        "colab_type": "code",
        "outputId": "b0dbfc97-af22-4b37-81d2-86f742834c29",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "l = Dense(128, 150)\n",
        "\n",
        "assert -0.05 < l.weights.mean() < 0.05 and 1e-3 < l.weights.std() < 1e-1,\\\n",
        "    \"The initial weights must have zero mean and small variance. \"\\\n",
        "    \"If you know what you're doing, remove this assertion.\"\n",
        "assert -0.05 < l.biases.mean() < 0.05, \"Biases must be zero mean. Ignore if you have a reason to do otherwise.\"\n",
        "\n",
        "# To test the outputs, we explicitly set weights with fixed values. DO NOT DO THAT IN ACTUAL NETWORK!\n",
        "l = Dense(3,4)\n",
        "\n",
        "x = np.linspace(-1,1,2*3).reshape([2,3])\n",
        "l.weights = np.linspace(-1,1,3*4).reshape([3,4])\n",
        "l.biases = np.linspace(-1,1,4)\n",
        "\n",
        "assert np.allclose(l.forward(x),np.array([[ 0.07272727,  0.41212121,  0.75151515,  1.09090909],\n",
        "                                          [-0.90909091,  0.08484848,  1.07878788,  2.07272727]]))\n",
        "print(\"Well done!\")"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Well done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AWRlFVvI4Xha",
        "colab_type": "code",
        "outputId": "4e941801-ee78-4c65-a3ca-2e2fd9d32187",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# To test the grads, we use gradients obtained via finite differences\n",
        "\n",
        "x = np.linspace(-1,1,10*32).reshape([10,32])\n",
        "l = Dense(32,64,learning_rate=0)\n",
        "\n",
        "numeric_grads = eval_numerical_gradient(lambda x: l.forward(x).sum(),x)\n",
        "grads = l.backward(x,np.ones([10,64]))\n",
        "\n",
        "assert np.allclose(grads,numeric_grads,rtol=1e-3,atol=0), \"input gradient does not match numeric grad\"\n",
        "print(\"Well done!\")"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Well done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ylrz3jAz4Xhf",
        "colab_type": "code",
        "outputId": "14a11e97-fa0c-49db-b61b-3aa836d4a7c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#test gradients w.r.t. params\n",
        "def compute_out_given_wb(w,b):\n",
        "    l = Dense(32,64,learning_rate=1)\n",
        "    l.weights = np.array(w)\n",
        "    l.biases = np.array(b)\n",
        "    x = np.linspace(-1,1,10*32).reshape([10,32])\n",
        "    return l.forward(x)\n",
        "    \n",
        "def compute_grad_by_params(w,b):\n",
        "    l = Dense(32,64,learning_rate=1)\n",
        "    l.weights = np.array(w)\n",
        "    l.biases = np.array(b)\n",
        "    x = np.linspace(-1,1,10*32).reshape([10,32])\n",
        "    l.backward(x,np.ones([10,64]) / 10.)\n",
        "    return w - l.weights, b - l.biases\n",
        "    \n",
        "w,b = np.random.randn(32,64), np.linspace(-1,1,64)\n",
        "\n",
        "numeric_dw = eval_numerical_gradient(lambda w: compute_out_given_wb(w,b).mean(0).sum(),w )\n",
        "numeric_db = eval_numerical_gradient(lambda b: compute_out_given_wb(w,b).mean(0).sum(),b )\n",
        "grad_w,grad_b = compute_grad_by_params(w,b)\n",
        "\n",
        "assert np.allclose(numeric_dw,grad_w,rtol=1e-3,atol=0), \"weight gradient does not match numeric weight gradient\"\n",
        "assert np.allclose(numeric_db,grad_b,rtol=1e-3,atol=0), \"weight gradient does not match numeric weight gradient\"\n",
        "print(\"Well done!\")"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Well done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dx2ZNxym4Xhj",
        "colab_type": "text"
      },
      "source": [
        "### The loss function\n",
        "\n",
        "Since we want to predict probabilities, it would be logical for us to define softmax nonlinearity on top of our network and compute loss given predicted probabilities. However, there is a better way to do so.\n",
        "\n",
        "If you write down the expression for crossentropy as a function of softmax logits (a), you'll see:\n",
        "\n",
        "$$ loss = - log \\space {e^{a_{correct}} \\over {\\underset i \\sum e^{a_i} } } $$\n",
        "\n",
        "If you take a closer look, ya'll see that it can be rewritten as:\n",
        "\n",
        "$$ loss = - a_{correct} + log {\\underset i \\sum e^{a_i} } $$\n",
        "\n",
        "It's called Log-softmax and it's better than naive log(softmax(a)) in all aspects:\n",
        "* Better numerical stability\n",
        "* Easier to get derivative right\n",
        "* Marginally faster to compute\n",
        "\n",
        "So why not just use log-softmax throughout our computation and never actually bother to estimate probabilities.\n",
        "\n",
        "Here you are! We've defined the both loss functions for you so that you could focus on neural network part."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "olYMfbFJ4Xhl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def softmax_crossentropy_with_logits(logits,reference_answers):\n",
        "    \"\"\"Compute crossentropy from logits[batch,n_classes] and ids of correct answers\"\"\"\n",
        "    logits_for_answers = logits[np.arange(len(logits)),reference_answers]\n",
        "    \n",
        "    xentropy = - logits_for_answers + np.log(np.sum(np.exp(logits),axis=-1))\n",
        "    \n",
        "    return xentropy\n",
        "\n",
        "def grad_softmax_crossentropy_with_logits(logits,reference_answers):\n",
        "    \"\"\"Compute crossentropy gradient from logits[batch,n_classes] and ids of correct answers\"\"\"\n",
        "    ones_for_answers = np.zeros_like(logits)\n",
        "    ones_for_answers[np.arange(len(logits)),reference_answers] = 1\n",
        "    \n",
        "    softmax = np.exp(logits) / np.exp(logits).sum(axis=-1,keepdims=True)\n",
        "    \n",
        "    return (- ones_for_answers + softmax) / logits.shape[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGO8uukn4Xhn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "logits = np.linspace(-1,1,500).reshape([50,10])\n",
        "answers = np.arange(50)%10\n",
        "\n",
        "softmax_crossentropy_with_logits(logits,answers)\n",
        "grads = grad_softmax_crossentropy_with_logits(logits,answers)\n",
        "numeric_grads = eval_numerical_gradient(lambda l: softmax_crossentropy_with_logits(l,answers).mean(),logits)\n",
        "\n",
        "assert np.allclose(numeric_grads,grads,rtol=1e-3,atol=0), \"The reference implementation has just failed. Someone has just changed the rules of math.\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMByYpoE4Xhq",
        "colab_type": "text"
      },
      "source": [
        "### Full network\n",
        "\n",
        "Now let's combine what we've just built into a working neural network. As we announced, we're gonna use this monster to classify handwritten digits, so let's get them loaded."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9V1hkGF4Xhr",
        "colab_type": "code",
        "outputId": "93d68a8d-332b-446d-8590-07aebfaa9380",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from preprocessed_mnist import load_dataset\n",
        "X_train, y_train, X_val, y_val, X_test, y_test = load_dataset(flatten=True)\n",
        "\n",
        "plt.figure(figsize=[6,6])\n",
        "for i in range(4):\n",
        "    plt.subplot(2,2,i+1)\n",
        "    plt.title(\"Label: %i\"%y_train[i])\n",
        "    plt.imshow(X_train[i].reshape([28,28]),cmap='gray');"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAF1CAYAAADx1LGMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x432 with 4 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5e5nr2x4Xhu",
        "colab_type": "text"
      },
      "source": [
        "We'll define network as a list of layers, each applied on top of previous one. In this setting, computing predictions and training becomes trivial."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "is9obi_P4Xhv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "network = []\n",
        "network.append(Dense(X_train.shape[1],100))\n",
        "network.append(ReLU())\n",
        "network.append(Dense(100,200))\n",
        "network.append(ReLU())\n",
        "network.append(Dense(200,10))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mHNBIJXe4Xhx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def forward(network, X):\n",
        "    \"\"\"\n",
        "    Compute activations of all network layers by applying them sequentially.\n",
        "    Return a list of activations for each layer. \n",
        "    Make sure last activation corresponds to network logits.\n",
        "    \"\"\"\n",
        "    activations = []\n",
        "    input = X\n",
        "\n",
        "    # <your code here>\n",
        "    for activation in network:\n",
        "      activations.append(activation.forward(input))\n",
        "      input = activations[-1] # input of next layer will be activation of previous layer\n",
        "    \n",
        "    assert len(activations) == len(network)\n",
        "    return activations\n",
        "\n",
        "def predict(network,X):\n",
        "    \"\"\"\n",
        "    Compute network predictions.\n",
        "    \"\"\"\n",
        "    logits = forward(network,X)[-1]\n",
        "    return logits.argmax(axis=-1)\n",
        "\n",
        "def train(network,X,y):\n",
        "    \"\"\"\n",
        "    Train your network on a given batch of X and y.\n",
        "    You first need to run forward to get all layer activations.\n",
        "    Then you can run layer.backward going from last to first layer.\n",
        "    \n",
        "    After you called backward for all layers, all Dense layers have already made one gradient step.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Get the layer activations\n",
        "    layer_activations = forward(network,X)\n",
        "    layer_inputs = [X]+layer_activations  #layer_input[i] is an input for network[i]\n",
        "    logits = layer_activations[-1]\n",
        "    \n",
        "    # Compute the loss and the initial gradient\n",
        "    loss = softmax_crossentropy_with_logits(logits,y)\n",
        "    loss_grad = grad_softmax_crossentropy_with_logits(logits,y)\n",
        "    \n",
        "    # <your code: propagate gradients through the network>\n",
        "    for layer_num in range(len(network))[::-1]:\n",
        "      layer = network[layer_num]\n",
        "      loss_grad = layer.backward(layer_inputs[layer_num], loss_grad)\n",
        "    \n",
        "    return np.mean(loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pjPgynVW4Xh3",
        "colab_type": "text"
      },
      "source": [
        "Instead of tests, we provide you with a training loop that prints training and validation accuracies on every epoch.\n",
        "\n",
        "If your implementation of forward and backward are correct, your accuracy should grow from 90~93% to >97% with the default network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVRUtr634Xh6",
        "colab_type": "text"
      },
      "source": [
        "### Training loop\n",
        "\n",
        "As usual, we split data into minibatches, feed each such minibatch into the network and update weights."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GY1RJiQT4Xh_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
        "    assert len(inputs) == len(targets)\n",
        "    if shuffle:\n",
        "        indices = np.random.permutation(len(inputs))\n",
        "    for start_idx in tqdm_utils.tqdm_notebook_failsafe(range(0, len(inputs) - batchsize + 1, batchsize)):\n",
        "        if shuffle:\n",
        "            excerpt = indices[start_idx:start_idx + batchsize]\n",
        "        else:\n",
        "            excerpt = slice(start_idx, start_idx + batchsize)\n",
        "        yield inputs[excerpt], targets[excerpt]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HF1aA2_H4XiE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from IPython.display import clear_output\n",
        "train_log = []\n",
        "val_log = []\n",
        "log_old = pd.DataFrame(columns=['Epoch', 'Train_Acc_old', 'Valid_Acc_old', 'Time_old'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1hQtN-F4XiS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "outputId": "f8760fae-b0f6-4f29-badc-f43fa561cd07"
      },
      "source": [
        "a = time.time()\n",
        "for epoch in range(25):\n",
        "\n",
        "    for x_batch,y_batch in iterate_minibatches(X_train,y_train,batchsize=32,shuffle=True):\n",
        "        train(network,x_batch,y_batch)\n",
        "    \n",
        "    train_log.append(np.mean(predict(network,X_train)==y_train))\n",
        "    val_log.append(np.mean(predict(network,X_val)==y_val))\n",
        "    temp = pd.DataFrame({'Epoch': [epoch+1], 'Train_Acc_old': [np.mean(predict(network,X_train)==y_train)], 'Valid_Acc_old': [np.mean(predict(network,X_val)==y_val)], 'Time_old': [time.time()-a]})\n",
        "    log_old = log_old.append(temp)\n",
        "    clear_output()\n",
        "    print(\"Epoch\",epoch)\n",
        "    print(\"Train accuracy:\",train_log[-1])\n",
        "    print(\"Val accuracy:\",val_log[-1])\n",
        "    plt.plot(train_log,label='train accuracy')\n",
        "    plt.plot(val_log,label='val accuracy')\n",
        "    plt.legend(loc='best')\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "print(\"Time taken for 25 epochs is \", time.time()-a)"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 24\n",
            "Train accuracy: 1.0\n",
            "Val accuracy: 0.9799\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Time taken for 25 epochs is  121.71880340576172\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9ovPasq4Xif",
        "colab_type": "text"
      },
      "source": [
        "### Peer-reviewed assignment\n",
        "\n",
        "Congratulations, you managed to get this far! There is just one quest left undone, and this time you'll get to choose what to do.\n",
        "\n",
        "\n",
        "#### Option I: initialization\n",
        "* Implement Dense layer with Xavier initialization as explained [here](http://bit.ly/2vTlmaJ)\n",
        "\n",
        "To pass this assignment, you must conduct an experiment showing how xavier initialization compares to default initialization on deep networks (5+ layers).\n",
        "\n",
        "\n",
        "#### Option II: regularization\n",
        "* Implement a version of Dense layer with L2 regularization penalty: when updating Dense Layer weights, adjust gradients to minimize\n",
        "\n",
        "$$ Loss = Crossentropy + \\alpha \\cdot \\underset i \\sum {w_i}^2 $$\n",
        "\n",
        "To pass this assignment, you must conduct an experiment showing if regularization mitigates overfitting in case of abundantly large number of neurons. Consider tuning $\\alpha$ for better results.\n",
        "\n",
        "#### Option III: optimization\n",
        "* Implement a version of Dense layer that uses momentum/rmsprop or whatever method worked best for you last time.\n",
        "\n",
        "Most of those methods require persistent parameters like momentum direction or moving average grad norm, but you can easily store those params inside your layers.\n",
        "\n",
        "To pass this assignment, you must conduct an experiment showing how your chosen method performs compared to vanilla SGD.\n",
        "\n",
        "### General remarks\n",
        "_Please read the peer-review guidelines before starting this part of the assignment._\n",
        "\n",
        "In short, a good solution is one that:\n",
        "* is based on this notebook\n",
        "* runs in the default course environment with Run All\n",
        "* its code doesn't cause spontaneous eye bleeding\n",
        "* its report is easy to read.\n",
        "\n",
        "_Formally we can't ban you from writing boring reports, but if you bored your reviewer to death, there's noone left alive to give you the grade you want._\n",
        "\n",
        "\n",
        "### Bonus assignments\n",
        "\n",
        "As a bonus assignment (no points, just swag), consider implementing Batch Normalization ([guide](https://gab41.lab41.org/batch-normalization-what-the-hey-d480039a9e3b)) or Dropout ([guide](https://medium.com/@amarbudhiraja/https-medium-com-amarbudhiraja-learning-less-to-learn-better-dropout-in-deep-machine-learning-74334da4bfc5)). Note, however, that those \"layers\" behave differently when training and when predicting on test set.\n",
        "\n",
        "* Dropout:\n",
        "  * During training: drop units randomly with probability __p__ and multiply everything by __1/(1-p)__\n",
        "  * During final predicton: do nothing; pretend there's no dropout\n",
        "  \n",
        "* Batch normalization\n",
        "  * During training, it substracts mean-over-batch and divides by std-over-batch and updates mean and variance.\n",
        "  * During final prediction, it uses accumulated mean and variance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O1bQwF-xOBhc",
        "colab_type": "text"
      },
      "source": [
        "## Option 1 Xavier Initialization\n",
        "#### Changes to be made: grad is not random but a random sequence generated with mean 0, var = 2/(no of input + output layers)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ikP2HC0BLLS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Dense(Layer):\n",
        "    def __init__(self, input_units, output_units, learning_rate=0.1):\n",
        "        \"\"\"\n",
        "        A dense layer is a layer which performs a learned affine transformation:\n",
        "        f(x) = <W*x> + b\n",
        "        \"\"\"\n",
        "        self.learning_rate = learning_rate\n",
        "        \n",
        "        # initialize weights with small random numbers. We use normal initialization, \n",
        "        # but surely there is something better. Try this once you got it working: http://bit.ly/2vTlmaJ\n",
        "        self.weights = np.random.normal(loc=0.0, scale = np.sqrt(2/(input_units+output_units)), size = (input_units,output_units))\n",
        "        self.biases = np.zeros(output_units)\n",
        "        \n",
        "    def forward(self,input):\n",
        "        \"\"\"\n",
        "        Perform an affine transformation:\n",
        "        f(x) = <W*x> + b\n",
        "        \n",
        "        input shape: [batch, input_units]\n",
        "        output shape: [batch, output units]\n",
        "        \"\"\"\n",
        "        \n",
        "        #return #<your code here>\n",
        "        return np.dot(input, self.weights) + self.biases\n",
        "    \n",
        "    def backward(self,input,grad_output):\n",
        "        \n",
        "        # compute d f / d x = d f / d dense * d dense / d x\n",
        "        # where d dense/ d x = weights transposed\n",
        "        #grad_input = #<your code here>\n",
        "        grad_input = np.dot(grad_output, self.weights.T)\n",
        "        \n",
        "        # compute gradient w.r.t. weights and biases\n",
        "        #grad_weights = #<your code here>\n",
        "        #grad_biases = #<your code here>\n",
        "        grad_weights = np.dot(input.T, grad_output)\n",
        "        grad_biases = grad_output.mean(axis=0)*input.shape[0]\n",
        "        \n",
        "        assert grad_weights.shape == self.weights.shape and grad_biases.shape == self.biases.shape\n",
        "        # Here we perform a stochastic gradient descent step. \n",
        "        # Later on, you can try replacing that with something better.\n",
        "        self.weights = self.weights - self.learning_rate * grad_weights\n",
        "        self.biases = self.biases - self.learning_rate * grad_biases\n",
        "        \n",
        "        return grad_input"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UgCS-ksJBj8S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#### New Network\n",
        "network = []\n",
        "network.append(Dense(X_train.shape[1],100))\n",
        "network.append(ReLU())\n",
        "network.append(Dense(100,200))\n",
        "network.append(ReLU())\n",
        "network.append(Dense(200,10))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gdsT5dlBB6BS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from IPython.display import clear_output\n",
        "new_train_log = []\n",
        "new_val_log = []\n",
        "log_new = pd.DataFrame(columns=['Epoch', 'Train_Acc_new', 'Valid_Acc_new', 'Time_new'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svQNzpoyC83i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "outputId": "0f1afb3d-8667-4289-bcb6-540a15e6b40b"
      },
      "source": [
        "a = time.time()\n",
        "for epoch in range(25):\n",
        "\n",
        "    for x_batch,y_batch in iterate_minibatches(X_train,y_train,batchsize=32,shuffle=True):\n",
        "        train(network,x_batch,y_batch)\n",
        "    \n",
        "    new_train_log.append(np.mean(predict(network,X_train)==y_train))\n",
        "    new_val_log.append(np.mean(predict(network,X_val)==y_val))\n",
        "    temp = pd.DataFrame({'Epoch': [epoch+1], 'Train_Acc_new': [np.mean(predict(network,X_train)==y_train)], 'Valid_Acc_new': [np.mean(predict(network,X_val)==y_val)], 'Time_new': [time.time()-a]})\n",
        "    log_new = log_new.append(temp)\n",
        "    \n",
        "    clear_output()\n",
        "    print(\"Epoch\",epoch)\n",
        "    print(\"Train accuracy:\",new_train_log[-1])\n",
        "    print(\"Val accuracy:\",new_val_log[-1])\n",
        "    plt.plot(new_train_log,label='train accuracy')\n",
        "    plt.plot(new_val_log,label='val accuracy')\n",
        "    plt.legend(loc='best')\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "print(\"Time taken for 25 epochs is \", time.time()-a)"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 24\n",
            "Train accuracy: 1.0\n",
            "Val accuracy: 0.981\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRCTpCyGEifx/beXjyYZ2nEEAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Time taken for 25 epochs is  122.58582782745361\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqimliVtN_eN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 824
        },
        "outputId": "aa261003-b97a-4460-b52d-373612bf717d"
      },
      "source": [
        "Log_Comparison = pd.merge(log_new, log_old, how = 'inner', on = 'Epoch')\n",
        "Log_Comparison = Log_Comparison[['Epoch', 'Train_Acc_new', 'Train_Acc_old', 'Valid_Acc_new', 'Valid_Acc_old', 'Time_new', 'Time_old']]\n",
        "Log_Comparison"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Epoch</th>\n",
              "      <th>Train_Acc_new</th>\n",
              "      <th>Train_Acc_old</th>\n",
              "      <th>Valid_Acc_new</th>\n",
              "      <th>Valid_Acc_old</th>\n",
              "      <th>Time_new</th>\n",
              "      <th>Time_old</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0.97218</td>\n",
              "      <td>0.95830</td>\n",
              "      <td>0.9663</td>\n",
              "      <td>0.9586</td>\n",
              "      <td>4.652004</td>\n",
              "      <td>4.105437</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>0.97884</td>\n",
              "      <td>0.96994</td>\n",
              "      <td>0.9695</td>\n",
              "      <td>0.9648</td>\n",
              "      <td>9.632750</td>\n",
              "      <td>8.727943</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>0.98382</td>\n",
              "      <td>0.97448</td>\n",
              "      <td>0.9741</td>\n",
              "      <td>0.9680</td>\n",
              "      <td>14.565332</td>\n",
              "      <td>13.591685</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>0.98800</td>\n",
              "      <td>0.98236</td>\n",
              "      <td>0.9754</td>\n",
              "      <td>0.9712</td>\n",
              "      <td>19.678319</td>\n",
              "      <td>18.539784</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>0.98972</td>\n",
              "      <td>0.98524</td>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.9747</td>\n",
              "      <td>24.682743</td>\n",
              "      <td>23.554674</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>6</td>\n",
              "      <td>0.99002</td>\n",
              "      <td>0.98636</td>\n",
              "      <td>0.9749</td>\n",
              "      <td>0.9737</td>\n",
              "      <td>29.411902</td>\n",
              "      <td>28.570326</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>7</td>\n",
              "      <td>0.99444</td>\n",
              "      <td>0.99106</td>\n",
              "      <td>0.9782</td>\n",
              "      <td>0.9774</td>\n",
              "      <td>34.282378</td>\n",
              "      <td>33.638394</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>8</td>\n",
              "      <td>0.99508</td>\n",
              "      <td>0.99170</td>\n",
              "      <td>0.9782</td>\n",
              "      <td>0.9746</td>\n",
              "      <td>39.068498</td>\n",
              "      <td>38.792502</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>9</td>\n",
              "      <td>0.99372</td>\n",
              "      <td>0.99316</td>\n",
              "      <td>0.9753</td>\n",
              "      <td>0.9760</td>\n",
              "      <td>43.805903</td>\n",
              "      <td>43.428479</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>10</td>\n",
              "      <td>0.99640</td>\n",
              "      <td>0.99392</td>\n",
              "      <td>0.9776</td>\n",
              "      <td>0.9745</td>\n",
              "      <td>48.712247</td>\n",
              "      <td>48.172403</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>11</td>\n",
              "      <td>0.99804</td>\n",
              "      <td>0.99458</td>\n",
              "      <td>0.9776</td>\n",
              "      <td>0.9746</td>\n",
              "      <td>53.460181</td>\n",
              "      <td>53.025596</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>12</td>\n",
              "      <td>0.99920</td>\n",
              "      <td>0.99656</td>\n",
              "      <td>0.9798</td>\n",
              "      <td>0.9778</td>\n",
              "      <td>58.279194</td>\n",
              "      <td>57.772728</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>13</td>\n",
              "      <td>0.99946</td>\n",
              "      <td>0.99792</td>\n",
              "      <td>0.9801</td>\n",
              "      <td>0.9778</td>\n",
              "      <td>63.195620</td>\n",
              "      <td>62.640209</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>14</td>\n",
              "      <td>0.99950</td>\n",
              "      <td>0.99828</td>\n",
              "      <td>0.9803</td>\n",
              "      <td>0.9773</td>\n",
              "      <td>68.028073</td>\n",
              "      <td>67.321737</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>15</td>\n",
              "      <td>0.99994</td>\n",
              "      <td>0.99910</td>\n",
              "      <td>0.9805</td>\n",
              "      <td>0.9789</td>\n",
              "      <td>72.986070</td>\n",
              "      <td>72.115477</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>16</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>0.99674</td>\n",
              "      <td>0.9813</td>\n",
              "      <td>0.9728</td>\n",
              "      <td>78.032732</td>\n",
              "      <td>77.018518</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>17</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>0.99956</td>\n",
              "      <td>0.9812</td>\n",
              "      <td>0.9778</td>\n",
              "      <td>83.074326</td>\n",
              "      <td>82.002295</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>18</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>0.99918</td>\n",
              "      <td>0.9809</td>\n",
              "      <td>0.9782</td>\n",
              "      <td>87.776806</td>\n",
              "      <td>87.026445</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>19</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>0.99982</td>\n",
              "      <td>0.9805</td>\n",
              "      <td>0.9796</td>\n",
              "      <td>92.472437</td>\n",
              "      <td>92.173728</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>20</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>0.99654</td>\n",
              "      <td>0.9807</td>\n",
              "      <td>0.9767</td>\n",
              "      <td>97.221632</td>\n",
              "      <td>97.111107</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>21</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>0.99994</td>\n",
              "      <td>0.9806</td>\n",
              "      <td>0.9793</td>\n",
              "      <td>102.188728</td>\n",
              "      <td>101.896038</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>22</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>0.9809</td>\n",
              "      <td>0.9794</td>\n",
              "      <td>107.315806</td>\n",
              "      <td>106.684224</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>23</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>0.9809</td>\n",
              "      <td>0.9796</td>\n",
              "      <td>112.377979</td>\n",
              "      <td>111.566512</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>24</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>0.99998</td>\n",
              "      <td>0.9814</td>\n",
              "      <td>0.9803</td>\n",
              "      <td>117.436583</td>\n",
              "      <td>116.507005</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>25</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>0.9810</td>\n",
              "      <td>0.9799</td>\n",
              "      <td>122.377656</td>\n",
              "      <td>121.497133</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Epoch  Train_Acc_new  Train_Acc_old  ...  Valid_Acc_old    Time_new    Time_old\n",
              "0      1        0.97218        0.95830  ...         0.9586    4.652004    4.105437\n",
              "1      2        0.97884        0.96994  ...         0.9648    9.632750    8.727943\n",
              "2      3        0.98382        0.97448  ...         0.9680   14.565332   13.591685\n",
              "3      4        0.98800        0.98236  ...         0.9712   19.678319   18.539784\n",
              "4      5        0.98972        0.98524  ...         0.9747   24.682743   23.554674\n",
              "5      6        0.99002        0.98636  ...         0.9737   29.411902   28.570326\n",
              "6      7        0.99444        0.99106  ...         0.9774   34.282378   33.638394\n",
              "7      8        0.99508        0.99170  ...         0.9746   39.068498   38.792502\n",
              "8      9        0.99372        0.99316  ...         0.9760   43.805903   43.428479\n",
              "9     10        0.99640        0.99392  ...         0.9745   48.712247   48.172403\n",
              "10    11        0.99804        0.99458  ...         0.9746   53.460181   53.025596\n",
              "11    12        0.99920        0.99656  ...         0.9778   58.279194   57.772728\n",
              "12    13        0.99946        0.99792  ...         0.9778   63.195620   62.640209\n",
              "13    14        0.99950        0.99828  ...         0.9773   68.028073   67.321737\n",
              "14    15        0.99994        0.99910  ...         0.9789   72.986070   72.115477\n",
              "15    16        1.00000        0.99674  ...         0.9728   78.032732   77.018518\n",
              "16    17        1.00000        0.99956  ...         0.9778   83.074326   82.002295\n",
              "17    18        1.00000        0.99918  ...         0.9782   87.776806   87.026445\n",
              "18    19        1.00000        0.99982  ...         0.9796   92.472437   92.173728\n",
              "19    20        1.00000        0.99654  ...         0.9767   97.221632   97.111107\n",
              "20    21        1.00000        0.99994  ...         0.9793  102.188728  101.896038\n",
              "21    22        1.00000        1.00000  ...         0.9794  107.315806  106.684224\n",
              "22    23        1.00000        1.00000  ...         0.9796  112.377979  111.566512\n",
              "23    24        1.00000        0.99998  ...         0.9803  117.436583  116.507005\n",
              "24    25        1.00000        1.00000  ...         0.9799  122.377656  121.497133\n",
              "\n",
              "[25 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRzFO5eNg2cy",
        "colab_type": "text"
      },
      "source": [
        "## By applying Xavier Initialization, the network converges faster and hence fewer epochs and time are needed. "
      ]
    }
  ]
}
